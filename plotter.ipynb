{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from zero_shot_time.data.splits import create_train_test_split, get_custom_train_test_split\n",
    "from zero_shot_time.data import get_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_maes(data, test_sets, scaler = 1):\n",
    "    mea = 0\n",
    "    last_meas =  0\n",
    "    for i in range(len(test_sets)):\n",
    "        slc = pd.concat(data[(i)*20:(i+1)*20]).groupby(['x']).median()['y']\n",
    "        pred = slc[-len(test_sets[i]):]\n",
    "        mea += np.mean(np.abs(pred - test_sets[i]))\n",
    "        last_meas += np.abs(pred - test_sets[i]).array[-1]\n",
    "    return mea / len(test_sets), mea / len(test_sets) / scaler\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_dataset_forecasts(configs, models, limit=5):\n",
    "    res = []\n",
    "    for dataset_name, (dataset_origin , scaler) in configs.items():\n",
    "        dataset, target = get_dataset(dataset_name=dataset_origin, sub_category=dataset_name)\n",
    "        if target is not None:\n",
    "            # Create train, validation, test split\n",
    "            _, train_sets, test_sets = create_train_test_split(dataset[\"train\"], dataset[\"test\"], target=target)\n",
    "        else:\n",
    "            _, train_sets, test_sets = get_custom_train_test_split(dataset, split_fraction=0.2)\n",
    "        for model_name in models:\n",
    "            data = []\n",
    "            with open(f'./results/{dataset_origin}_{dataset_name}_{model_name}.data.pickle', 'rb') as f:\n",
    "                preds = pickle.load(f)\n",
    "                f.close()\n",
    "            for p_idx, p in enumerate(preds):\n",
    "                for r_index, r in enumerate(p[0][0]):\n",
    "                    data.append(\n",
    "                            pd.DataFrame(\n",
    "                                    data={\n",
    "                                        'x': np.arange(len(r)).flatten().tolist(),\n",
    "                                        'y': r,\n",
    "                                        'v': [p_idx] * len(r),\n",
    "                                        'r': [r_index] * len(r)\n",
    "                                    }\n",
    "                            )\n",
    "                    )\n",
    "            assert len(data) == 20 * len(train_sets)\n",
    "            mae, maes =  compute_maes(data, test_sets, scaler)\n",
    "            print(dataset_name, model_name, mae, maes)\n",
    "            res += [{'dataset': dataset_name,\n",
    "                     'model': model_name,\n",
    "                     'mae': mae,\n",
    "                     'maes': maes}\n",
    "                    ]\n",
    "            for i in range(min(limit, len(test_sets))):\n",
    "                # Getting the 20 predictions\n",
    "                slc = pd.concat(data[(i)*20:(i+1)*20])\n",
    "                train_len = len(train_sets[i])\n",
    "                pred_len = len(test_sets[i])\n",
    "                # Plotting prediction, taking additional steps to 'connect' the lines\n",
    "                # Note that we plot the median!\n",
    "                seaborn.lineplot(slc[slc.x > train_len-2], x='x', y='y', errorbar='ci',\n",
    "                             estimator=np.median, color='blue')\n",
    "                # Plot historical data\n",
    "                plt.plot(np.arange(len(train_sets[i])), train_sets[i], color='black')\n",
    "                # Plot slightly transparent future data\n",
    "                plt.plot(np.arange(len(train_sets[i])-1, len(train_sets[i]) + pred_len), [train_sets[i][-1]] + test_sets[i], linewidth=2, alpha=0.7, color='orange')\n",
    "\n",
    "                # Set title of figure\n",
    "                plt.title(f'{dataset_name} {model_name} series: {i+1}')\n",
    "                plt.savefig(f'./plots/{dataset_name}_{model_name}_{i+1}.pdf')\n",
    "\n",
    "                # Show figure after saving it\n",
    "                plt.show()\n",
    "    return res\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting Darts results\n",
    "\n",
    "Then we consider the Darts result, plotting them with our defined function. We also write the plots to the [`./plots/`](./plots) directory.\n",
    "\n",
    "Additionally, we create a table that allows us to view the results in a single overview for the blog post!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scaler take from original repository\n",
    "darts_models = ['gpt2', 'gpt2-large', 'Llama-2-7b-hf', 'Llama-2-13b-hf']\n",
    "darts_dataset = {\n",
    "    'airpassenger': ('darts', 1.0),\n",
    "    'beer': ('darts', 1.0),\n",
    "}\n",
    "\n",
    "darts_result = plot_dataset_forecasts(darts_dataset, darts_models, limit=1)\n",
    "\n",
    "print(pd.DataFrame(darts_result)[['model', 'dataset', 'mae']].to_latex(index=False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting Monash results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scaler take from original repository\n",
    "monash_models = ['gpt2', 'gpt2-large', 'Llama-2-7b-hf']\n",
    "\n",
    "monash_datasets = {\n",
    "    'tourism_yearly': ('monash_tsf', 99456.0540551959), # https://github.com/ngruver/llmtime/blob/a9b451b9dad4b443d5c00652b39d922effd8870e/data/last_val_mae.csv?plain=1#L13\n",
    "    'nn5_weekly':('monash_tsf', 16.708553516113007), # https://github.com/ngruver/llmtime/blob/a9b451b9dad4b443d5c00652b39d922effd8870e/data/last_val_mae.csv?plain=1#L21\n",
    "}\n",
    "\n",
    "darts_result = plot_dataset_forecasts(monash_datasets, monash_models, limit=5)\n",
    "\n",
    "print(pd.DataFrame(darts_result)[['model', 'dataset', 'mae', 'maes']].pivot( index=[ 'model', 'dataset'], columns=[]).to_latex())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
